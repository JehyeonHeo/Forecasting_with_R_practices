# Chapter 3

```{r echo=FALSE, message=FALSE, warning=FALSE, Load_packages}

library(fpp2)

```

1. For the following series, find an appropriate Box-Cox transformation in order to stabilize the variance.

```{r echo=FALSE, message=FALSE, warning=FALSE, Question1}

# - usnetelec
lambda_usnetelec <- BoxCox.lambda(usnetelec)
print(c("Good value of lambda for usnetelec: ", 
        lambda_usnetelec))
autoplot(BoxCox(usnetelec, lambda_usnetelec))

# - usgdp
lambda_usgdp <- BoxCox.lambda(usgdp)
print(c("Good value of lambda for usgdp: ", 
      lambda_usgdp))
autoplot(BoxCox(usgdp, lambda_usgdp))

# - mcopper
lambda_mcopper <- BoxCox.lambda(mcopper)
print(c("Good value of lambda for mcopper: ", 
      lambda_mcopper))
autoplot(BoxCox(mcopper, lambda_mcopper))

# - enplanements
lambda_enplanements <- BoxCox.lambda(enplanements)
print(c("Good value of lambda for enplanements: ", 
      lambda_enplanements))
autoplot(BoxCox(enplanements, lambda_enplanements))

```


2. Why is a Box-Cox transformation unhelpful for the cangas data?

```{r echo=FALSE, message=FALSE, warning=FALSE, Question2}

autoplot(cangas)

lambda_cangas <- BoxCox.lambda(cangas)
autoplot(BoxCox(cangas, lambda_cangas))
# can see that Box-Cox transformation doesn't yield simpler model

```

3. What Box-Cox transformation would you select for your retail data (from Exercise 3 in Section 2.10)?

```{r echo=FALSE, message=FALSE, warning=FALSE, Question3}

retaildata <- xlsx::read.xlsx("retail.xlsx", sheetIndex = 1, startRow = 2)
myts <- ts(retaildata[,"A3349873A"], frequency=12, start=c(1982,4))

lambda_retail <- BoxCox.lambda(myts)
print(c("selected lambda:", lambda_retail))

fc_retail <- rwf(myts, 
                 drift = TRUE, 
                 lambda = lambda_retail,
                 h = 50,
                 level = 80)

fc_retail_biasadj <- rwf(myts, 
                         drift = TRUE, 
                         lambda = lambda_retail,
                         h = 50,
                         level = 80,
                         biasadj = TRUE)

autoplot(myts) +
  autolayer(fc_retail, series = "Simple Back Transform") +
  autolayer(fc_retail_biasadj$mean, series = "Bias Adjusted") +
  guides(colour = guide_legend(title = "Forecast"))
# It would be better to choose bias adjusted Box-Cox Transformation with lambda = 0.128

```

4. For each of the following series, make a graph of the data. If transforming seems appropriate, do so and describe the effect. dole, usdeaths, bricksq.

```{r echo=FALSE, message=FALSE, warning=FALSE, Question4}

autoplot(dole)
lambda_dole <- BoxCox.lambda(dole)
autoplot(BoxCox(dole, lambda_dole))
# For dole data, it looked like using Box-Cox Transformation would be better to see the pattern

autoplot(usdeaths)
lambda_usdeaths <- BoxCox.lambda(usdeaths)
autoplot(BoxCox(usdeaths, lambda_usdeaths))
# For usdeaths data, it looked like it is meaningless to transform it

autoplot(bricksq)
lambda_bricksq <- BoxCox.lambda(bricksq)
autoplot(BoxCox(bricksq, lambda_bricksq))
# For bricksq data, it looked like it is meaningless to transform it

```


5. Calculate the residuals from a seasonal naïve forecast applied to the quarterly Australian beer production data from 1992. Test if the residuals are white noise and normally distributed. What do you conclude?

```{r echo=FALSE, message=FALSE, warning=FALSE, Question5}

beer <- window(ausbeer, start=1992)
fc <- snaive(beer)
autoplot(fc)
res <- residuals(fc)
autoplot(res)

checkresiduals(fc)

# When just see the result of ACF, it looked like the residuals aren't white noise because of 4-lagged value. And Ljung-Box test shows that it is statistically signigicant. Therefore they aren't white noise. Also they aren't normally distributed.

```


6. Repeat the exercise for the WWWusage and bricksq data. Use whichever of naive or snaive is more appropriate in each case.

```{r echo=FALSE, message=FALSE, warning=FALSE, Question6}

snaive_www <- snaive(WWWusage, h = 15)
autoplot(snaive_www)
checkresiduals(snaive_www)

naive_www <- naive(WWWusage)
autoplot(naive_www)
checkresiduals(naive_www)

# When just see the result of ACF, it looked like the residuals aren't white noise because of out of bound values. And Ljung-Box test shows that they are statistically signigicant for both of methods. Therefore they aren't white noise. And the distribution of residuals aren't normal, too.
# If I need to choose between above 2 methods, I will choose naive method because there isn't any particular seasonal pattern in the data and the Q values of Ljung-Box test were same for both methods.

snaive_bricksq <- snaive(bricksq)
autoplot(snaive_bricksq)
checkresiduals(snaive_bricksq)

naive_bricksq <- naive(bricksq)
autoplot(naive_bricksq)
checkresiduals(naive_bricksq)

# For both of methods, Ljung-Box test results show that the residuals aren't white noise. And the residuals didn't have normal distribution, too.
# If I need to choose between above 2 methods, I will choose snaive method because I can see the seasonality in the data and the Q value of Ljung-Box of snaive methods was less than the value of naive method.

```

7. Are the following statements true or false? Explain your answer.

 a. Good forecast methods should have normally distributed residuals.
 b. A model with small residuals will give good forecasts.
 c. The best measure of forecast accuracy is MAPE.
 d. If your model doesn't forecast well, you should make it more complicated.
 e. Always choose the model with the best forecast accuracy as measured on the test set.

answer 

- a. It is useful to have normally distributed residuals, but it isn't necessary for good forecast methods to have them.

 - b. Good forecast methods don't need to have small residuals. But their residuals should be uncorrelated and have zero mean.
 
 - c. The best measure of forecast accuracy is different case by case.
 
 - d. More complicated forecast method doesn't guarantee better forecast.
 
 - e. When choosing the model, whether the residuals have zero mean and they are uncorrelated with each other is also considered. Simply choosing best accuracy model isn't good.

8. For your retail time series (from Exercise 3 in Section 2.10):

```{r echo=FALSE, message=FALSE, warning=FALSE, Question8}

# a. Split the data into two parts using

myts.train <- window(myts, end=c(2010,12))
myts.test <- window(myts, start=2011)

# b. Check that your data have been split appropriately by producing the following plot.

autoplot(myts) + 
  autolayer(myts.train, series="Training") +
  autolayer(myts.test, series="Test")

# c. Calculate forecasts using snaive applied to myts.train.

snaive_myts_train <- snaive(myts.train)

# d. Compare the accuracy of your forecasts against the actual values stored in myts.test.

accuracy(snaive_myts_train, myts.test)
 
# e. Check the residuals. Do the residuals appear to be uncorrelated and normally distributed?

checkresiduals(snaive_myts_train)
# residuals were correlated with each other and not normally distributed

# f. How sensitive are the accuracy measures to the training/test split?

# Compared between the ratios of the test set error to the train set error, it looked like Mean Error is highly sensitive, RMSE, MAE, MPE, MASE are sensitive and MAPE and ACF1 aren't much sensitive.
# I don't know whether this method is what the author wanted to solve this question.

```

9. vn contains quarterly visitor nights (in millions) from 1998-2015 for eight regions of Australia.

```{r echo=FALSE, message=FALSE, warning=FALSE, Question9}

# a. Use window() to create three training sets for vn[,"Melbourne"], omitting the last 1, 2 and 3 years; call these train1, train2, and train3, respectively. For example train1 <- window(vn[, "Melbourne"], end = c(2014, 4)).
vn_Melbourne_train1 <- window(vn[, "Melbourne"], end = c(2014, 4))
vn_Melbourne_train2 <- window(vn[, "Melbourne"], end = c(2013, 4))
vn_Melbourne_train3 <- window(vn[, "Melbourne"], end = c(2012, 4))

# b. Compute one year of forecasts for each training set using the snaive() method. 
snaive_vn_Melbourne_train1 <- snaive(vn_Melbourne_train1, h = 4)
snaive_vn_Melbourne_train2 <- snaive(vn_Melbourne_train2, h = 4)
snaive_vn_Melbourne_train3 <- snaive(vn_Melbourne_train3, h = 4)

# c. Use accuracy() to compare the MAPE over the three test sets. Comment on these.
vn_Melbourne_test1 <- window(vn[, "Melbourne"], start = c(2015, 1), end = c(2015, 4))
vn_Melbourne_test2 <- window(vn[, "Melbourne"], start = c(2014, 1), end = c(2014, 4))
vn_Melbourne_test3 <- window(vn[, "Melbourne"], start = c(2013, 1), end = c(2013, 4))

accuracy(snaive_vn_Melbourne_train1, vn_Melbourne_test1)
writeLines("")
accuracy(snaive_vn_Melbourne_train2, vn_Melbourne_test2)
writeLines("")
accuracy(snaive_vn_Melbourne_train3, vn_Melbourne_test3)

# MAPE was smallest for 2015 prediction and biggest for 2014 prediction. MAPE became smaller in 2013 prediction, but it wan't smaller than the one for 2015.
```


10. Use the Dow Jones index (data set dowjones) to do the following:

```{r echo=FALSE, message=FALSE, warning=FALSE, Question10}

# a. Produce a time plot of the series.
autoplot(dowjones)

# b. Produce forecasts using the drift method and plot them.
drift_dj <- rwf(dowjones, drift = TRUE)
autoplot(drift_dj)

# c. Show that the forecasts are identical to extending the line drawn between the first and last observations.
dj_x <- c(1, 78)
dj_y <- c(dowjones[1], dowjones[78])
lm_dj <- lm(dj_y ~ dj_x)

# It looked like ggplot2 package isn't compatible to graphics package that autoplot function, made of ggplot2 can't be worked with graphic function like abline, etc. When I tried it, 'plot.new has not been called yet' error appeared.
autoplot(drift_dj) +
  geom_abline(intercept = lm_dj$coefficients[1],
              slope = lm_dj$coefficients[2],
              colour = "red")

autoplot(drift_dj) +
  geom_line(aes(x = c(1, 78),
                y = dowjones[c(1, 78)]), 
            colour = "red")

# d. Try using some of the other benchmark functions to forecast the same data set. Which do you think is best? Why?
checkresiduals(drift_dj)

mean_dj <- meanf(dowjones)
autoplot(mean_dj)

naive_dj <- naive(dowjones)
autoplot(naive_dj)
checkresiduals(naive_dj)

snaive_dj <- snaive(dowjones, h = 10)
autoplot(snaive_dj)
# I think that naive method is best because it is really difficult to predict stock price with past observations.  I think that it is safer to just take the value of last observation using naive method

```


11. Consider the daily closing IBM stock prices (data set ibmclose).

```{r echo=FALSE, message=FALSE, warning=FALSE, Question11}

# a. Produce some plots of the data in order to become familiar with it.
autoplot(ibmclose)

# b. Split the data into a training set of 300 observations and a test set of 69 observations.
ibm_train <- subset(ibmclose, end = 300)
ibm_test <- subset(ibmclose, start = 301)

# c. Try using various benchmark methods to forecast the training set and compare the results on the test set. Which method did best?
snaive_ibm <- snaive(ibm_train, h = 69)
naive_ibm <- naive(ibm_train, h = 69)
drift_ibm <- rwf(ibm_train, drift = TRUE, h = 69)
mean_ibm <- meanf(ibm_train, h = 69)

autoplot(snaive_ibm) +
  autolayer(ibm_test)
autoplot(naive_ibm) +
  autolayer(ibm_test)
autoplot(drift_ibm) +
  autolayer(ibm_test)
autoplot(mean_ibm) +
  autolayer(ibm_test)

writeLines("Snaive method")
accuracy(snaive_ibm, ibm_test)

writeLines("\nNaive method")
accuracy(naive_ibm, ibm_test)

writeLines("\nDrift method")
accuracy(drift_ibm, ibm_test)

writeLines("\nMean method")
accuracy(mean_ibm, ibm_test)

e_snaive_ibm <- ibm_test - snaive_ibm$mean
e_naive_ibm <- ibm_test - naive_ibm$mean
e_drift_ibm <- ibm_test - drift_ibm$mean
e_mean_ibm <- ibm_test - mean_ibm$mean

autoplot(e_snaive_ibm^2, series = "snaive method") +
  autolayer(e_naive_ibm^2, series = "naive method") +
  autolayer(e_drift_ibm^2, series = "drift method") +
  autolayer(e_mean_ibm^2, series = "mean method") +
  guides(colour = guide_legend(title = "Forecast")) +
  ggtitle("Errors of the forecast of closing IBM stock price") +
  ylab(expression(paste("erro", r^{2})))
# Drift method did best

# Time series cross-validation method of tsCV function don't use full data unless h = 1. For example, if usable data has 100 points and h = 3, tsCV predicts 101st point with 98 points, 102nd with 99 points and 103rd with 100 points. Therefore error result value of tsCV cannot help differing from the values of accuracy function. Accuracy function always get result from full data.
ibmclose %>% tsCV(forecastfunction = snaive, h = 69) ->  e_snaive_ibm_CV
ibmclose %>% tsCV(forecastfunction = naive, h = 69) ->  e_naive_ibm_CV
ibmclose %>% tsCV(forecastfunction = rwf, drift = TRUE, h = 69) ->  e_drift_ibm_CV
ibmclose %>% tsCV(forecastfunction = meanf, h = 69) ->  e_mean_ibm_CV

autoplot(subset(e_snaive_ibm_CV^2, start = 301), series = "snaive method") +
  autolayer(subset(e_naive_ibm_CV^2, start = 301), series = "naive method") +
  autolayer(subset(e_drift_ibm_CV^2, start = 301), series = "drift method") +
  autolayer(subset(e_mean_ibm_CV^2, start = 301), series = "mean method") +
  guides(colour = guide_legend(title = "Forecast")) +
  ggtitle("Errors of the forecast of closing IBM stock price",
          subtitle = "after using tsCV function") +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5)) +
  ylab(expression(paste("erro", r^{2})))
# If I choose to use tsCV function, I would've selected naive method because it yields smallest error.
  
# d. Check the residuals of your preferred method. Do they resemble white noise?
checkresiduals(naive_ibm)
checkresiduals(drift_ibm)

# No. Even I checked the residuals of naive method because the error values were similar to result of drift method, residuals weren't like white noise for both cases
```


12. Consider the sales of new one-family houses in the USA, Jan 1973 - Nov 1995 (data set hsales).

 a. Produce some plots of the data in order to become familiar with it.
 b. Split the hsales data set into a training set and a test set, where the test set is the last two years of data.
 c. Try using various benchmark methods to forecast the training set and compare the results on the test set.  Which method did best?
 d. Check the residuals of your preferred method. Do they resemble white noise?