# Chapter 11

```{r echo=FALSE, message=FALSE, warning=FALSE, Load_packages}

library(fpp2)
library(vars)
library(xlsx)

```

1. Use the tbats() function to model your retail time series.

```{r echo=FALSE, message=FALSE, warning=FALSE, Question1}

# a. Check the residuals and produce forecasts.
retail <- read.xlsx("retail.xlsx",
                    sheetIndex = 1,
                    startRow = 2)
retail.ts <- ts(retail[,"A3349873A"], 
                frequency=12, 
                start=c(1982,4))

retail_tbats <- tbats(retail.ts)
checkresiduals(retail_tbats)
# The residuals aren't like white noise. And they are right skewed.

fc_retail_tbats <- forecast(retail_tbats, h = 36)
autoplot(fc_retail_tbats)

# test accuracy using future data.
retail.new <- read.xlsx("8501011.xlsx", 
                        sheetName = "Data1", 
                        startRow = 10)
retail.new.ts <- ts(retail.new[, "A3349873A"],
                    start = c(1982, 4),
                    frequency = 12)
retail.new.test <- subset(
  retail.new.ts,
  start = length(retail.ts) + 1
  )

accuracy(fc_retail_tbats, retail.new.test)
# TBATS model was worse than Holt-Winters' or dynamic regression model. It was better than ARIMA model or seasonal naive method.

# b. Does this completely automated approach work for these data?
# Judging from the forecasts shown in plot, it looked like that TBATS model worked for these data well.

# c. Have you saved any degrees of freedom by using Fourier terms rather than seasonal differencing?
retail_tbats
# TBATS(0.126, {5, 0}, 1, {<12, 5>})
# TBATS(lambda, {p, q}, damping, {<freq, num of terms>})
# alpha - level(lt) smoothing parameter
# beta - trend(bt) smoothing parameter
# gamma1 - seasonal level(sj,t) smoothing parameter
# gamma2 - seasonal growth(s*j,t) smoothing parameter
# j - Fourier term(j times of base frequency)
# Seed states - initial state variables. In this case, there are 17. Maybe first and second of them are l0 and b0, 3rd to 7th are s1,0 to s5,0, 8th to 12th are s*1,0 to s*5,0, 13th to 17th are d-4 to d0. d is disturbance, and it can be thought of as white noise. d-4 to d0 are all 0s because errors before the first time point can be assumed as 0.
# When I used checkresiduals function, I could know that the degrees of freedom of the model are 27. Maybe 2 are related with level and trend, 5 are related with AR(5) and the rest 20 are related with seasonality. 20 = 2(smoothing parameters) x 2(cos, sin) x 5(number of terms).

# When I used ARIMA model for these data, ARIMA(1, 0, 2)(0, 1, 1)[12] with drift model was chosen. This model used seasonal differencing to model seasonal component.
# When I used checkresiduals function, I could know that the degrees of freedom of the model are 5. Maybe 1 for AR(1), 2 for MA(2), 1 for seasonal MA(1) and final 1 for drift coefficient.

# When just compared degrees of freedom related with seasonal component, tbats model saved 18 dofs. Having more dofs means more flexibility. TBATS model got more dofs, therefore the model can deal with more number of different seasonalities at a time.

```


2. Consider the weekly data on US finished motor gasoline products supplied (thousands of barrels per day) (series gasoline):

```{r echo=FALSE, message=FALSE, warning=FALSE, Question2}

# a. Fit a TBATS model to these data.
gasoline_tbats <- tbats(gasoline)

# b. Check the residuals and produce forecasts.
checkresiduals(gasoline_tbats)
# The residuals aren't like white noise.

fc_gasoline_tbats <- forecast(gasoline_tbats)
autoplot(fc_gasoline_tbats)
# It looked like TBATS model isn't fitted well.

# c. Could you model these data using any of the other methods we have considered in this book?
# I think that as I did in Question 4 of Chapter 9, dynamic regression model is the best for the data. I think so because regression can deal with the piecewise trends in the data, and ARIMA model can be fitted for residuals well.

```


3. Experiment with using nnetar() on your retail data and other data we have considered in previous chapters.

```{r echo=FALSE, message=FALSE, warning=FALSE, Question3}

retail_nnetar <- nnetar(
  retail.ts, lambda = BoxCox.lambda(retail.ts)
  )
fc_retail_nnetar <- forecast(retail_nnetar, h = 36)
autoplot(fc_retail_nnetar)

# test accuracy using future data.
accuracy(fc_retail_nnetar, retail.new.test)
# It is better than all methods I tried so far, including Holt-Winters'.

# experiment with ibmclose data.
ibmclose_nnetar <- nnetar(ibmclose)
fc_ibmclose_nnetar <- forecast(ibmclose_nnetar)
autoplot(fc_ibmclose_nnetar)
# Even neural network method yielded naive-method like result. It looked like there wan't any rule in lagged values.

# experiment with usmelec data.
usmelec_nnetar <- nnetar(
  usmelec, lambda = BoxCox.lambda(usmelec)
  )

fc_usmelec_nnetar <- forecast(
  usmelec_nnetar, h = 12*4
)

autoplot(fc_usmelec_nnetar)

# get the latest figures
usmelec.new <- read.csv("MER_T07_02A.csv", sep = ",")
usmelec.new[, "Year"] <- as.numeric(substr(usmelec.new[, "YYYYMM"], 1, 4))
usmelec.new[, "Month"] <- as.numeric(
  substr(usmelec.new[, "YYYYMM"], 5, 6)
  )
usmelec.new <- subset(
  usmelec.new, 
  Description == "Electricity Net Generation Total, All Sectors", 
  select = c("Year", "Month", "Value")
  )
usmelec.new <- subset(usmelec.new, Month != 13)
usmelec.new[, "Value"] <- as.numeric(
  as.character(usmelec.new[, "Value"])
  )/1000
usmelec.new.ts <- ts(
  as.numeric(usmelec.new[, "Value"]), 
  start = c(1973, 1), 
  frequency = 12
  )

# get accuracy
accuracy(fc_usmelec_nnetar, usmelec.new.ts)
# Without ME and MPE, all the other errors show that neural network model is better than ARIMA model to forecast the usmelec data.

autoplot(fc_usmelec_nnetar) +
  autolayer(window(usmelec.new.ts, start = c(2013, 7))) +
  scale_x_continuous(limits = c(2010, 2019)) +
  scale_y_continuous(limits = c(250, 450))
# It looked like neural network model was fitted well to the data.

```

## Deal with complex seasonality in data example

```{r echo=FALSE, message=FALSE, warning=FALSE, complex_seasonality}

# plot calls data
p1 <- autoplot(calls) +
  ylab("Call volume") + 
  xlab("Weeks") +
  scale_x_continuous(breaks=seq(1,33,by=2))

p2 <- autoplot(window(calls, end=4)) +
  ylab("Call volume") + 
  xlab("Weeks") +
  scale_x_continuous(minor_breaks = seq(1,4,by=0.2))

gridExtra::grid.arrange(p1,p2)

# 1. STL with multiple seasonal periods(Use mstl function)
# - used development version of forecast package to use mstl function(2018/2/22). Run below 3 rows of codes without #s to download it.
# install.packages("devtools")
# library(devtools)
# devtools::install_github("robjhyndman/forecast")
calls %>%
  mstl() %>%
  autoplot()

# 2. Dynamic harmonic regression with multiple seasonal periods(Use auto.arima function)
calls_autoarima <- auto.arima(
  # model seasonal component using Fourier terms, not using ARIMA model(set seasonal option as FALSE).
  calls, seasonal=FALSE, lambda=0,
  # For K in fourier function, use vector instead of a number because calls data are msts(multi-seasonal time series). There are 2 seasonal frequencies in the data that the vector needs to specify 2 numbers of Fourier terms.
  xreg=fourier(calls, K=c(10,10))
  )

fc_calls_autoarima <- forecast(
  calls_autoarima, 
  xreg=fourier(calls, K=c(10,10), h=2*169)
  )

autoplot(fc_calls_autoarima, include=5*169) +
  ylab("Call volume") + 
  xlab("Weeks")

# 3. TBATS model(Use tbats function)
# - One drawback of TBATS models is that they can be very slow to estimate, especially with long time series. So I will consider a subset of the calls data to save time.
calls_tbats <- calls %>%
  subset(start=length(calls)-2000) %>%
  tbats()

fc_calls_tbats <- forecast(calls_tbats, h=2*169)

autoplot(fc_calls_tbats, include=5*169) +
  ylab("Call volume") + xlab("Weeks")

# 4. Dynamic harmonic regression with multiple seasonal periods and covariates(Use auto.arima function)
# I'll use elecdemand data in this case because I can use several time series in the data as covariates.
elecdemand %>%
  as.data.frame %>%
  ggplot(aes(x=Temperature, y=Demand)) + 
    geom_point() +
    xlab("Temperature (degrees Celsius)") +
    ylab("Demand (GW)")

cooling <- pmax(elecdemand[,"Temperature"], 18)

elecdemand_dreg <- auto.arima(
  elecdemand[,"Demand"],
  # To forecast total electricity demand, use 2 time series related with temperature as covariates.
  xreg = cbind(fourier(elecdemand, c(10,10,0)),
               heating=elecdemand[,"Temperature"],
               cooling=cooling)
  )

# I'll do scenario forecasting. Therefore I'll use a repeat of the last two days of temperatures to generate future possible demand values.
temps.new <- subset(
  elecdemand[, c(1:3)],
  start=NROW(elecdemand)-2*48+1
  )

cooling.new <- pmax(temps.new, 18)

# forecast temperature using Fourier terms
fc_elecdemand_dreg <- forecast(
  elecdemand_dreg, 
  xreg=cbind(fourier(temps.new, c(10,10,0)),
             heating=temps.new, 
             cooling=pmax(cooling.new,18))
  )

autoplot(fc_elecdemand_dreg, include=14*48)
checkresiduals(fc_elecdemand_dreg)
# Although the short-term forecasts look reasonable, this is a very crude model for a complicated process. 

```

