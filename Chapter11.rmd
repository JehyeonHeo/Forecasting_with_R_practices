# Chapter 11

```{r echo=FALSE, message=FALSE, warning=FALSE, Load_packages}

library(fpp2)
library(vars)
library(xlsx)

```

1. Use the tbats() function to model your retail time series.

```{r echo=FALSE, message=FALSE, warning=FALSE, Question1}

# a. Check the residuals and produce forecasts.
retail <- read.xlsx("retail.xlsx",
                    sheetIndex = 1,
                    startRow = 2)
retail.ts <- ts(retail[,"A3349873A"], 
                frequency=12, 
                start=c(1982,4))

retail_tbats <- tbats(retail.ts)
checkresiduals(retail_tbats)
# The residuals aren't like white noise. And they are right skewed.

fc_retail_tbats <- forecast(retail_tbats, h = 36)
autoplot(fc_retail_tbats)

# test accuracy using future data.
retail.new <- read.xlsx("8501011.xlsx", 
                        sheetName = "Data1", 
                        startRow = 10)
retail.new.ts <- ts(retail.new[, "A3349873A"],
                    start = c(1982, 4),
                    frequency = 12)
retail.new.test <- subset(
  retail.new.ts,
  start = length(retail.ts) + 1
  )

accuracy(fc_retail_tbats, retail.new.test)
# TBATS model was worse than Holt-Winters' or dynamic regression model. It was better than ARIMA model or seasonal naive method.

# b. Does this completely automated approach work for these data?
# Judging from the forecasts shown in plot, it looked like that TBATS model worked for these data well.

# c. Have you saved any degrees of freedom by using Fourier terms rather than seasonal differencing?
retail_tbats
# TBATS(0.126, {5, 0}, 1, {<12, 5>})
# TBATS(lambda, {p, q}, damping, {<freq, num of terms>})
# alpha - level(lt) smoothing parameter
# beta - trend(bt) smoothing parameter
# gamma1 - seasonal level(sj,t) smoothing parameter
# gamma2 - seasonal growth(s*j,t) smoothing parameter
# j - Fourier term(j times of base frequency)
# Seed states - initial state variables. In this case, there are 17. Maybe first and second of them are l0 and b0, 3rd to 7th are s1,0 to s5,0, 8th to 12th are s*1,0 to s*5,0, 13th to 17th are d-4 to d0. d is disturbance, and it can be thought of as white noise. d-4 to d0 are all 0s because errors before the first time point can be assumed as 0.
# When I used checkresiduals function, I could know that the degrees of freedom of the model are 27. Maybe 2 are related with level and trend, 5 are related with AR(5) and the rest 20 are related with seasonality. 20 = 2(smoothing parameters) x 2(cos, sin) x 5(number of terms).

# When I used ARIMA model for these data, ARIMA(1, 0, 2)(0, 1, 1)[12] with drift model was chosen. This model used seasonal differencing to model seasonal component.
# When I used checkresiduals function, I could know that the degrees of freedom of the model are 5. Maybe 1 for AR(1), 2 for MA(2), 1 for seasonal MA(1) and final 1 for drift coefficient.

# When just compared degrees of freedom related with seasonal component, tbats model saved 18 dofs. Having more dofs means more flexibility. TBATS model got more dofs, therefore the model can deal with more number of different seasonalities at a time.

```


2. Consider the weekly data on US finished motor gasoline products supplied (thousands of barrels per day) (series gasoline):

```{r echo=FALSE, message=FALSE, warning=FALSE, Question2}

# a. Fit a TBATS model to these data.
gasoline_tbats <- tbats(gasoline)

# b. Check the residuals and produce forecasts.
checkresiduals(gasoline_tbats)
# The residuals aren't like white noise.

fc_gasoline_tbats <- forecast(gasoline_tbats)
autoplot(fc_gasoline_tbats)
# It looked like TBATS model isn't fitted well.

# c. Could you model these data using any of the other methods we have considered in this book?
# I think that as I did in Question 4 of Chapter 9, dynamic regression model is the best for the data. I think so because regression can deal with the piecewise trends in the data, and ARIMA model can be fitted for residuals well.

```


3. Experiment with using nnetar() on your retail data and other data we have considered in previous chapters.

```{r echo=FALSE, message=FALSE, warning=FALSE, Question3}

retail_nnetar <- nnetar(
  retail.ts, lambda = BoxCox.lambda(retail.ts)
  )
fc_retail_nnetar <- forecast(retail_nnetar, h = 36)
autoplot(fc_retail_nnetar)

# test accuracy using future data.
accuracy(fc_retail_nnetar, retail.new.test)
# It is better than all methods I tried so far, including Holt-Winters'.

# experiment with ibmclose data.
ibmclose_nnetar <- nnetar(ibmclose)
fc_ibmclose_nnetar <- forecast(ibmclose_nnetar)
autoplot(fc_ibmclose_nnetar)
# Even neural network method yielded naive-method like result. It looked like there wan't any rule in lagged values.

# experiment with usmelec data.
usmelec_nnetar <- nnetar(
  usmelec, lambda = BoxCox.lambda(usmelec)
  )

fc_usmelec_nnetar <- forecast(
  usmelec_nnetar, h = 12*4
)

autoplot(fc_usmelec_nnetar)

# get the latest figures
usmelec.new <- read.csv("MER_T07_02A.csv", sep = ",")
usmelec.new[, "Year"] <- as.numeric(substr(usmelec.new[, "YYYYMM"], 1, 4))
usmelec.new[, "Month"] <- as.numeric(
  substr(usmelec.new[, "YYYYMM"], 5, 6)
  )
usmelec.new <- subset(
  usmelec.new, 
  Description == "Electricity Net Generation Total, All Sectors", 
  select = c("Year", "Month", "Value")
  )
usmelec.new <- subset(usmelec.new, Month != 13)
usmelec.new[, "Value"] <- as.numeric(
  as.character(usmelec.new[, "Value"])
  )/1000
usmelec.new.ts <- ts(
  as.numeric(usmelec.new[, "Value"]), 
  start = c(1973, 1), 
  frequency = 12
  )

# get accuracy
accuracy(fc_usmelec_nnetar, usmelec.new.ts)
autoplot(fc_usmelec_nnetar) +
  autolayer(window(usmelec.new.ts, start = c(2013, 7))) +
  scale_x_continuous(limits = c(2010, 2019)) +
  scale_y_continuous(limits = c(250, 450))
# It looked like neural network model was fitted well to the data.

```

