# Chapter 11

```{r echo=FALSE, message=FALSE, warning=FALSE, Load_packages}

library(fpp2)
library(vars)
library(xlsx)

```

1. Use the tbats() function to model your retail time series.

```{r echo=FALSE, message=FALSE, warning=FALSE, Question1}

# a. Check the residuals and produce forecasts.
retail <- read.xlsx("retail.xlsx",
                    sheetIndex = 1,
                    startRow = 2)
retail.ts <- ts(retail[,"A3349873A"], 
                frequency=12, 
                start=c(1982,4))

retail_tbats <- tbats(retail.ts)
checkresiduals(retail_tbats)
# The residuals aren't like white noise. And they are right skewed.

fc_retail_tbats <- forecast(retail_tbats, h = 36)
autoplot(fc_retail_tbats)

# test accuracy using future data.
retail.new <- read.xlsx("8501011.xlsx", 
                        sheetName = "Data1", 
                        startRow = 10)
retail.new.ts <- ts(retail.new[, "A3349873A"],
                    start = c(1982, 4),
                    frequency = 12)
retail.new.test <- subset(
  retail.new.ts,
  start = length(retail.ts) + 1
  )

accuracy(fc_retail_tbats, retail.new.test)
# TBATS model was worse than Holt-Winters' or dynamic regression model. It was better than ARIMA model or seasonal naive method.

# b. Does this completely automated approach work for these data?
# Judging from the forecasts shown in plot, it looked like that TBATS model worked for these data well.

# c. Have you saved any degrees of freedom by using Fourier terms rather than seasonal differencing?
retail_tbats
# TBATS(0.126, {5, 0}, 1, {<12, 5>})
# TBATS(lambda, {p, q}, damping, {<freq, num of terms>})
# alpha - level(lt) smoothing parameter
# beta - trend(bt) smoothing parameter
# gamma1 - seasonal level(sj,t) smoothing parameter
# gamma2 - seasonal growth(s*j,t) smoothing parameter
# j - Fourier term(j times of base frequency)
# Seed states - initial state variables. In this case, there are 17. Maybe first and second of them are l0 and b0, 3rd to 7th are s1,0 to s5,0, 8th to 12th are s*1,0 to s*5,0, 13th to 17th are d-4 to d0. d is disturbance, and it can be thought of as white noise. d-4 to d0 are all 0s because errors before the first time point can be assumed as 0.
# When I used checkresiduals function, I could know that the degrees of freedom of the model are 27. Maybe 2 are related with level and trend, 5 are related with AR(5) and the rest 20 are related with seasonality. 20 = 2(smoothing parameters) x 2(cos, sin) x 5(number of terms).

# When I used ARIMA model for these data, ARIMA(1, 0, 2)(0, 1, 1)[12] with drift model was chosen. This model used seasonal differencing to model seasonal component.
# When I used checkresiduals function, I could know that the degrees of freedom of the model are 5. Maybe 1 for AR(1), 2 for MA(2), 1 for seasonal MA(1) and final 1 for drift coefficient.

# When just compared degrees of freedom related with seasonal component, tbats model saved 18 dofs. Having more dofs means more flexibility. TBATS model got more dofs, therefore the model can deal with more number of different seasonalities at a time.

```


2. Consider the weekly data on US finished motor gasoline products supplied (thousands of barrels per day) (series gasoline):

```{r echo=FALSE, message=FALSE, warning=FALSE, Question2}

# a. Fit a TBATS model to these data.
gasoline_tbats <- tbats(gasoline)

# b. Check the residuals and produce forecasts.
checkresiduals(gasoline_tbats)
# The residuals aren't like white noise.

fc_gasoline_tbats <- forecast(gasoline_tbats)
autoplot(fc_gasoline_tbats)
# It looked like TBATS model isn't fitted well.

# c. Could you model these data using any of the other methods we have considered in this book?
# I think that as I did in Question 4 of Chapter 9, dynamic regression model is the best for the data. I think so because regression can deal with the piecewise trends in the data, and ARIMA model can be fitted for residuals well.

```


3. Experiment with using nnetar() on your retail data and other data we have considered in previous chapters.

```{r echo=FALSE, message=FALSE, warning=FALSE, Question3}

retail_nnetar <- nnetar(
  retail.ts, lambda = BoxCox.lambda(retail.ts)
  )
fc_retail_nnetar <- forecast(retail_nnetar, h = 36)
autoplot(fc_retail_nnetar)

# test accuracy using future data.
accuracy(fc_retail_nnetar, retail.new.test)
# It is better than all methods I tried so far, including Holt-Winters'.

# experiment with ibmclose data.
ibmclose_nnetar <- nnetar(ibmclose)
fc_ibmclose_nnetar <- forecast(ibmclose_nnetar)
autoplot(fc_ibmclose_nnetar)
# Even neural network method yielded naive-method like result. It looked like there wan't any rule in lagged values.

# experiment with usmelec data.
usmelec_nnetar <- nnetar(
  usmelec, lambda = BoxCox.lambda(usmelec)
  )

fc_usmelec_nnetar <- forecast(
  usmelec_nnetar, h = 12*4
)

autoplot(fc_usmelec_nnetar)

# get the latest figures
usmelec.new <- read.csv("MER_T07_02A.csv", sep = ",")
usmelec.new[, "Year"] <- as.numeric(substr(usmelec.new[, "YYYYMM"], 1, 4))
usmelec.new[, "Month"] <- as.numeric(
  substr(usmelec.new[, "YYYYMM"], 5, 6)
  )
usmelec.new <- subset(
  usmelec.new, 
  Description == "Electricity Net Generation Total, All Sectors", 
  select = c("Year", "Month", "Value")
  )
usmelec.new <- subset(usmelec.new, Month != 13)
usmelec.new[, "Value"] <- as.numeric(
  as.character(usmelec.new[, "Value"])
  )/1000
usmelec.new.ts <- ts(
  as.numeric(usmelec.new[, "Value"]), 
  start = c(1973, 1), 
  frequency = 12
  )

# get accuracy
accuracy(fc_usmelec_nnetar, usmelec.new.ts)
# Without ME and MPE, all the other errors show that neural network model is better than ARIMA model to forecast the usmelec data.

autoplot(fc_usmelec_nnetar) +
  autolayer(window(usmelec.new.ts, start = c(2013, 7))) +
  scale_x_continuous(limits = c(2010, 2019)) +
  scale_y_continuous(limits = c(250, 450))
# It looked like neural network model was fitted well to the data.

```

## Deal with complex seasonality in data example

```{r echo=FALSE, message=FALSE, warning=FALSE, complex_seasonality}

# plot calls data
p1 <- autoplot(calls) +
  ylab("Call volume") + 
  xlab("Weeks") +
  scale_x_continuous(breaks=seq(1,33,by=2))

p2 <- autoplot(window(calls, end=4)) +
  ylab("Call volume") + 
  xlab("Weeks") +
  scale_x_continuous(minor_breaks = seq(1,4,by=0.2))

gridExtra::grid.arrange(p1,p2)

# 1. STL with multiple seasonal periods(Use mstl function)
# - used development version of forecast package to use mstl function(2018/2/22). Run below 3 rows of codes without #s to download it.
# install.packages("devtools")
# library(devtools)
# devtools::install_github("robjhyndman/forecast")
calls %>%
  mstl() %>%
  autoplot()

# 2. Dynamic harmonic regression with multiple seasonal periods(Use auto.arima function)
calls_autoarima <- auto.arima(
  # model seasonal component using Fourier terms, not using ARIMA model(set seasonal option as FALSE).
  calls, seasonal=FALSE, lambda=0,
  # For K in fourier function, use vector instead of a number because calls data are msts(multi-seasonal time series). There are 2 seasonal frequencies in the data that the vector needs to specify 2 numbers of Fourier terms.
  xreg=fourier(calls, K=c(10,10))
  )

fc_calls_autoarima <- forecast(
  calls_autoarima, 
  xreg=fourier(calls, K=c(10,10), h=2*169)
  )

autoplot(fc_calls_autoarima, include=5*169) +
  ylab("Call volume") + 
  xlab("Weeks")

# 3. TBATS model(Use tbats function)
# - One drawback of TBATS models is that they can be very slow to estimate, especially with long time series. So I will consider a subset of the calls data to save time.
calls_tbats <- calls %>%
  subset(start=length(calls)-2000) %>%
  tbats()

fc_calls_tbats <- forecast(calls_tbats, h=2*169)

autoplot(fc_calls_tbats, include=5*169) +
  ylab("Call volume") + xlab("Weeks")

# 4. Dynamic harmonic regression with multiple seasonal periods and covariates(Use auto.arima function)
# I'll use elecdemand data in this case because I can use several time series in the data as covariates.
elecdemand %>%
  as.data.frame %>%
  ggplot(aes(x=Temperature, y=Demand)) + 
    geom_point() +
    xlab("Temperature (degrees Celsius)") +
    ylab("Demand (GW)")

cooling <- pmax(elecdemand[,"Temperature"], 18)

elecdemand_dreg <- auto.arima(
  elecdemand[,"Demand"],
  # To forecast total electricity demand, use 2 time series related with temperature as covariates.
  xreg = cbind(fourier(elecdemand, c(10,10,0)),
               heating=elecdemand[,"Temperature"],
               cooling=cooling)
  )

# I'll do scenario forecasting. Therefore I'll use a repeat of the last two days of temperatures to generate future possible demand values.
temps.new <- subset(
  elecdemand[, c(1:3)],
  start=NROW(elecdemand)-2*48+1
  )

cooling.new <- pmax(temps.new, 18)

# forecast temperature using Fourier terms
fc_elecdemand_dreg <- forecast(
  elecdemand_dreg, 
  xreg=cbind(fourier(temps.new, c(10,10,0)),
             heating=temps.new, 
             cooling=pmax(cooling.new,18))
  )

autoplot(fc_elecdemand_dreg, include=14*48)
checkresiduals(fc_elecdemand_dreg)
# Although the short-term forecasts look reasonable, this is a very crude model for a complicated process. 

```

## Use VAR model for forecasting

```{r echo=FALSE, message=FALSE, warning=FALSE, VAR_model}

# make a VAR model for forecasting US consumption
VARselect(
  uschange[, 1:2], lag.max=8, type="const"
  )[["selection"]]
# There is a large discrepancy between the VAR(5) selected by the AIC and the VAR(1) selected by the BIC. As a result I'm going to fit a VAR(1) first, as selected by the BIC. And then I'll increase lag order one by one to get a model whose residuals are uncorrelated.

# used consumption and income as variables.
uschange_var1 <- VAR(uschange[,1:2], p=1, type="const")
summary(uschange_var1)

# computes the multivariate asymptotic Portmanteau test for serially correlated errors.
serial.test(
  uschange_var1, lags.pt=10, type="PT.asymptotic"
  )
# p value is more than 0.05. This model can be used.

uschange_var2 <- VAR(uschange[,1:2], p=2, type="const")
summary(uschange_var2)

serial.test(
  uschange_var2, lags.pt=10, type="PT.asymptotic"
  )
# p value is less than 0.05. Try 1 higher order.

uschange_var3 <- VAR(uschange[,1:2], p=3, type="const")
summary(uschange_var3)

serial.test(
  uschange_var3, lags.pt=10, type="PT.asymptotic"
  )
# p value is more than 0.05. This model can be used.

# try making model using production variable too.
VARselect(
  uschange[, 1:3], lag.max=8, type="const"
  )[["selection"]]
# try VAR(1) model first.

uschange_var1.v3 <- VAR(
  uschange[,1:3], p=1, type="const"
  )
summary(uschange_var1.v3)

serial.test(
  uschange_var1.v3, lags.pt=10, type="PT.asymptotic"
  )
# p value is less than 0.05. Try 1 higher order model.

uschange_var2.v3 <- VAR(
  uschange[,1:3], p=2, type="const"
  )
summary(uschange_var2.v3)

serial.test(
  uschange_var2.v3, lags.pt=10, type="PT.asymptotic"
  )
# p value is less than 0.05. Try 1 higher order model.

uschange_var3.v3 <- VAR(
  uschange[,1:3], p=3, type="const"
  )
summary(uschange_var3.v3)

serial.test(
  uschange_var3.v3, lags.pt=10, type="PT.asymptotic"
  )
# p value is more than 0.05. This model can be used.

# forecast using usable models.
forecast(uschange_var1) %>% autoplot()
forecast(uschange_var3) %>% autoplot()
forecast(uschange_var3.v3) %>% autoplot()
# It looked like VAR(3) model is a little better than VAR(1).

```

## Get prediction interval of Neural Network model using simulations or PI option.

```{r echo=FALSE, message=FALSE, warning=FALSE, nnetar_PI}

# simulation of 9 possible future sample paths for the sunspot data. 
sunspotarea_nnetar <- nnetar(sunspotarea, lambda=0)

sim <- ts(
  matrix(0, nrow=30L, ncol=9L), 
  start=end(sunspotarea)[1L]+1L
  )

for(i in seq(9)){
  sim[,i] <- simulate(sunspotarea_nnetar, nsim=30L)
}

autoplot(sunspotarea) + autolayer(sim)

# if PI=TRUE option is used in forecast.nnetar function, prediction interval calculated by simulations can be shown. But it takes more time to do the option.
forecast(sunspotarea_nnetar, PI=TRUE, h=30) %>%
  autoplot()

# try using ibmclose data.
forecast(nnetar(ibmclose), PI=TRUE, h=30) %>%
  autoplot()
# even neural network model couldn't yield meaningful forecasts.

```

## Bootstrapping and Bagging

```{r echo=FALSE, message=FALSE, warning=FALSE, Bootstrapping_and_Bagging}

# Bootstrap the residuals of debitcards time series in order to simulate future values of the series using a model.
# bld.mbb.bootstrap function - Box-Cox And Loess-Based Decomposition with Moving Block Bootstrap.
# generate 10 bootstrapped versions.
bootseries <- bld.mbb.bootstrap(debitcards, 10) %>%
  as.data.frame %>% 
  ts(start=2000, frequency=12)

autoplot(debitcards) +
  autolayer(bootseries, colour=TRUE) +
  autolayer(debitcards, colour=FALSE) +
  ylab("Bootstrapped series") + 
  guides(colour="none")

# Get prediction intervals from bootstrapped series
# generate 1000 bootstrapped versions.
nsim <- 1000L
sim <- bld.mbb.bootstrap(debitcards, nsim)

# For each of these series, I'm going to fit an ETS model and simulate one sample path from that model. The estimated parameters will be different. And the point forecasts will be different, too.
h <- 36L
debitcards.future <- matrix(0, nrow=nsim, ncol=h)

# put each simulation's point forecasts in each row of debitcards.future matrix.
# It takes lots of time to run below loop.
for(i in seq(nsim)){
  debitcards.future[i,] <- simulate(
    ets(sim[[i]]), nsim=h
    )
}

# I can make debitcards.future using map function of purrr package, too.
debitcards.future <- purrr::map(
  as.list(bld.mbb.bootstrap(debitcards, 1000L)), 
  function(x){forecast(ets(x), h=h)$mean}
  ) %>%
  # change the class from list to vector.
  unlist()

# change the class from vector to matrix. I couldn't get 1000 x 24 matrix by directly change the class to matrix from list. Therefore I made the format as vector first and then change it as matrix designating the number of rows and columns.
dim(debitcards.future) <- c(1000, 24)

# Finally, take the means and quantiles of these simulated sample paths to form point forecasts and prediction intervals.
# tsp function extract start time, end time and the frequency of data.
# h.start = first time point of forecast horizon.
h.start <- tsp(debitcards)[2] + 1/12

# calculate each column's mean, lower limit and upper limit.
fc_debitcards_sim <- structure(
  list(
    mean = ts(
      colMeans(debitcards.future), 
      start=h.start, 
      frequency=12
      ),
    lower = ts(
      apply(debitcards.future, 2, quantile, prob=0.025),
      start=h.start, 
      frequency=12
      ),
    upper = ts(
      apply(debitcards.future, 2, quantile, prob=0.975),
      start=h.start, 
      frequency=12
      ),
    level=95),
  class="forecast")

# get point forecasts and PI using forecast function.
fc_debitcards_ets <- forecast(ets(debitcards), h=h)

# plot the results
autoplot(debitcards) +
  ggtitle("Monthly retail debit card usage in Iceland") +
  xlab("Year") + ylab("million ISK") +
  autolayer(fc_debitcards_sim, series="Simulated") +
  autolayer(fc_debitcards_ets, series="ETS")
# Can see that the PI of bootstraped simulations yielded larger interval than the one obtained from an ETS model applied directly to the original data.

# Bagging(Bootstrap AGGregatING)
# - produce forecasts from each of the additional bootstrapped time series, and average the resulting forecasts. We can get better forecasts than if we simply forecast the original time series directly.
fc_debitcards_sim.sets <- purrr::map(
  as.list(bootseries), 
  function(x){forecast(ets(x))$mean}
  ) %>%
  # map function returns a list. Therefore to make it as ts object, transform it to data.frame first.
    as.data.frame() %>%
    ts(frequency=12, start=h.start)
# fc_debitcards_sim : ETS modeling for each 1000 bootstrapped series, generate 1000 sets of point forecasts. And then aggregate the 1000 sets by getting mean, lower and upper limits of the sets.
# fc_debitcards_ets : ETS modeling from original data only, generate one set of point forecasts.
# fc_debitcards_ets.mean : ETS modeling for each 10 bootstrapped series, generate 10 sets of point forecasts.

# get aggregated mean forecasts from fc_debitcards_sim.sets. 
fc_debitcards_sim2 <- structure(
  list(
    mean = ts(
        # The rows of fc_debitcards_sim.sets are time points of forecast horizon. The columns are bootstrapped versions. Therefore get means for each row.
      rowMeans(fc_debitcards_sim.sets), 
      start=h.start, 
      frequency=12
      )
    ),
  class="forecast")

# plot the results.
autoplot(debitcards) +
  autolayer(bootseries, colour=TRUE) +
  autolayer(fc_debitcards_sim.sets, colour=TRUE) +
  autolayer(debitcards, colour=FALSE) +
  autolayer(fc_debitcards_sim2$mean, colour=FALSE) +
  ylab("Bootstrapped series") +
  guides(colour="none")

# But the whole procedure can be handled with the baggedETS function.
fc_debitcards_ets <- debitcards %>% 
  ets %>% 
  forecast(h=36)
fc_debitcards_bagged.ets <- debitcards %>% 
  baggedETS %>% 
  forecast(h=36)

autoplot(debitcards) +
  autolayer(fc_debitcards_bagged.ets$mean, 
            series="BaggedETS") +
  autolayer(fc_debitcards_ets$mean, series="ETS") +
  guides(colour=guide_legend(title="Forecasts"))
# In this case, the forecast difference of the 2 functions is small.
# By default, 100 bootstrapped series are used. And on average, baggedets gives better forecasts than just applying ets directly. But it is slower because a lot more computation is required.
# baggedModel function can be used instead. fn option can be designated as ets or auto.arima.

```

